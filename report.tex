\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsmath,amsfonts}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{float}
\usepackage{microtype}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[left=1in,right=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}


\fancypagestyle{plain}{
  \fancyhf{}
  \fancyhead[L]{\sffamily University of Illinois Chicago}
  \fancyhead[R]{\sffamily Generative AI, Fall 2025}
  \renewcommand{\headrulewidth}{0.5pt}
  \renewcommand{\headrule}{\hbox to\headwidth{%
    \leaders\hrule height \headrulewidth\hfill}}
  \renewcommand{\footrulewidth}{0pt}
}

\title{Dynamic and Quantization-Aware Attention Reuse for Diffusion Transformers}
\author{Gautham Satyanarayana}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Diffusion Transformers (DiTs) have become a dominant architecture for large-scale generative modeling, but their inference remains costly due to quadratic self-attention and large key-value (KV) caches that are recomputed at each timestep. We present \textbf{DQAR} (Dynamic and Quantization-Aware Attention Reuse), a unified framework that combines entropy- and SNR-based attention reuse with low-bit quantized KV caching. Our approach uses information-theoretic signals to decide when cached attention maps can be safely reused, while quantizing stored tensors to minimize VRAM and I/O costs. We implement a lightweight policy network ($<0.5$M parameters) that fuses entropy, SNR, and latent norms into data-driven reuse decisions, along with layer-aware scheduling that aligns reuse intensity with the diffusion timeline. Experiments on a dummy DiT model demonstrate that DQAR achieves up to 37\% inference speedup while maintaining adaptive reuse control through entropy gating, achieving 90\% of static reuse performance with quality-preserving gates.
\end{abstract}

\section{Introduction}
Diffusion Transformers (DiTs) have emerged as a powerful architecture for generative modeling, combining the scalability of transformers with the denoising diffusion process~\cite{peebles2023dit}. However, their inference remains computationally expensive due to two primary factors: (1) quadratic self-attention complexity, and (2) large key-value (KV) caches that must be recomputed for each diffusion timestep.

Recent work has addressed these challenges from two complementary angles. Attention compression methods~\cite{yuan2024ditfastattn} have demonstrated that attention maps exhibit strong temporal redundancy during sampling, enabling partial reuse between steps. Concurrently, post-training quantization techniques~\cite{wu2024ptq4dit,chen2024qdit} have tackled quantization challenges unique to diffusion transformers, specifically addressing salient activation channels and timestep-dependent distributions.

This project proposes \textbf{DQAR} (Dynamic and Quantization-Aware Attention Reuse), a unified framework that integrates both approaches: (1) using information-theoretic signals (entropy and SNR) to decide when to reuse cached attention, and (2) quantizing reused KV tensors to minimize VRAM and I/O costs. This coupling creates an adaptive, compute-efficient inference pipeline for DiTs.

\subsection{Related Work}

\textbf{Attention Compression and Reuse.} Yuan et al.~\cite{yuan2024ditfastattn} proposed three strategies---Window Attention with Residual Sharing, Attention Sharing Across Timesteps, and CFG Branch Sharing---to exploit redundancy in DiTs. Their method achieved significant FLOP reductions while maintaining image quality, demonstrating that attention maps can be reused safely once convergence begins.

\textbf{Quantization in Diffusion Transformers.} PTQ4DiT~\cite{wu2024ptq4dit} introduced channel-wise salience balancing (CSB) and Spearman's $\rho$-guided calibration to stabilize post-training quantization across timesteps. Q-DiT~\cite{chen2024qdit} extended this by proposing automatic granularity allocation and dynamic activation quantization. EfficientDM~\cite{he2023efficientdm} explored quantization-aware fine-tuning for general diffusion models, showing that hybrid quantization can yield strong efficiency-quality trade-offs.

\textbf{Motivation for DQAR.} Existing approaches treat compression and quantization independently. Our framework integrates both by using entropy and SNR to gate reuse decisions while quantizing cached tensors, creating an adaptive inference pipeline that balances compute savings with quality preservation.

\section{Method}

\subsection{System Overview}
DQAR consists of five interconnected components that integrate directly into standard DDIM or DPM-Solver samplers. Figure~\ref{fig:architecture} illustrates the overall architecture. At each diffusion step, the controller coordinates reuse decisions based on entropy/SNR gating, policy network scores, and layer scheduling constraints.

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.85, transform shape]
    % Main controller box
    \draw[thick, fill=blue!5] (0,0) rectangle (12,5);
    \node at (6,4.6) {\textbf{DQARController (Orchestration Layer)}};

    % Sub-components
    \draw[fill=orange!20] (0.3,2.8) rectangle (2.8,4.2);
    \node[align=center] at (1.55,3.5) {\small Policy\\Network};

    \draw[fill=green!20] (3.2,2.8) rectangle (5.7,4.2);
    \node[align=center] at (4.45,3.5) {\small Entropy/SNR\\Gate};

    \draw[fill=purple!20] (6.1,2.8) rectangle (8.6,4.2);
    \node[align=center] at (7.35,3.5) {\small Quantized\\KV Cache};

    \draw[fill=red!20] (9.0,2.8) rectangle (11.7,4.2);
    \node[align=center] at (10.35,3.5) {\small Layer\\Scheduler};

    % Lower components
    \draw[fill=yellow!20] (0.3,0.6) rectangle (3.5,2.2);
    \node[align=center] at (1.9,1.4) {\small CSB Quantizer};

    \draw[fill=cyan!20] (4.0,0.6) rectangle (8.0,2.2);
    \node[align=center] at (6.0,1.4) {\small Stats: Entropy, SNR, L2 Norm};

    \draw[fill=gray!20] (8.5,0.6) rectangle (11.7,2.2);
    \node[align=center] at (10.1,1.4) {\small Config System};

    % Arrows
    \draw[->, thick] (1.55,2.8) -- (1.55,2.2);
    \draw[->, thick] (4.45,2.8) -- (4.45,2.2);
    \draw[->, thick] (7.35,2.8) -- (6.5,2.2);

\end{tikzpicture}
\caption{DQAR system architecture showing the main controller and its sub-components.}
\label{fig:architecture}
\end{figure}

\subsection{Entropy \& SNR-Based Reuse Gate}
At each timestep $t$, we compute the attention entropy and latent signal-to-noise ratio to determine reuse eligibility. The attention entropy aggregates information across all heads:
\begin{equation}
H_t = -\frac{1}{H} \sum_{h=1}^{H} \sum_{i,j} A_t^{(h)}(i,j) \log\left(A_t^{(h)}(i,j) + \epsilon\right)
\end{equation}
where $A_t^{(h)}$ is the attention map for head $h$ at timestep $t$, and $\epsilon$ is a small constant for numerical stability.

The signal-to-noise ratio is estimated directly from the diffusion timestep using the noise schedule, avoiding the need for access to the clean latent $x_0$:
\begin{equation}
\text{SNR}_t \approx \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t}
\end{equation}
where $\bar{\alpha}_t$ is the cumulative noise schedule at timestep $t$ (e.g., $\bar{\alpha}_t = \cos^2(\pi t / 2T)$ for cosine schedule). When the model's noise prediction $\epsilon_\theta$ is available, DDIM's $\hat{x}_0$ estimate can provide a more accurate SNR estimate.

The adaptive threshold scales with prompt length to account for the natural relationship between context length and attention entropy:
\begin{equation}
\tau(p) = \tau_{\text{base}} \cdot \sqrt{p / 16}
\end{equation}
Longer prompts tend to have more focused attention distributions, so we \emph{increase} the threshold to permit more reuse opportunities.

Attention is reused only if $H_t < \tau(p)$ and $\text{SNR}_t \in [a, b]$.

\subsection{Quantized KV Caching}
Cached K/V tensors are stored in 8-bit integer form using symmetric quantization:
\begin{equation}
K_q = \text{clip}\left(\text{round}\left(\frac{K}{s_K}\right), -127, 127\right), \quad s_K = \frac{\max|K|}{127}
\end{equation}

Two modes are supported: (1) per-tensor scaling for speed, and (2) per-channel scaling for higher fidelity. A mixed-precision variant keeps $K$ in FP16 and quantizes only $V$. The quantizer implements Channel-wise Salience Balancing (CSB) inspired by PTQ4DiT~\cite{wu2024ptq4dit} to handle activation outliers.

\subsection{Dynamic Reuse Policy}
A lightweight MLP ($<0.5$M parameters) predicts the probability of successful reuse:
\begin{equation}
p_{\text{reuse}} = P_\theta\left(\text{concat}(H_t, \text{SNR}_t, \|x_t\|_2, t/T)\right)
\end{equation}

The policy network takes as input:
\begin{itemize}
    \item Attention entropy $H_t$
    \item Signal-to-noise ratio $\text{SNR}_t$
    \item Latent L2 norm $\|x_t\|_2$
    \item Normalized step progress $t/T$
    \item Prompt length (for text-conditional models)
\end{itemize}

The policy is trained offline on cached inference traces using supervised learning. Our training pipeline:
\begin{enumerate}
    \item Collects traces by running diffusion with varying reuse configurations
    \item Computes rewards based on quality preservation (latent similarity to baseline) and compute savings
    \item Trains the policy MLP using gradient descent with reward-weighted labels
\end{enumerate}
This enables data-driven decisions without modifying the diffusion model weights.

\subsection{Layer Scheduling}
Early timesteps reuse only shallow blocks (where entropy is high and signal weak), while later timesteps reuse deeper blocks (where convergence stabilizes attention). This scheduling aligns reuse intensity with the diffusion timeline:

\begin{itemize}
    \item \textbf{Early phase} ($t < 0.3T$): Only layers $0$ to $L/3$ eligible
    \item \textbf{Mid phase} ($0.3T \leq t < 0.7T$): Layers $0$ to $2L/3$ eligible
    \item \textbf{Late phase} ($t \geq 0.7T$): All layers eligible for reuse
\end{itemize}

\subsection{Integration}
DQAR modules integrate directly into standard samplers. Each diffusion step:
\begin{enumerate}
    \item Computes entropy and SNR for the current state
    \item Consults the reuse gate and policy network
    \item If approved, retrieves quantized K/V and dequantizes
    \item Otherwise, recomputes attention and updates caches
\end{enumerate}

Algorithm~\ref{alg:dqar} summarizes the per-layer decision process.

\begin{algorithm}[H]
\caption{DQAR Reuse Decision}
\label{alg:dqar}
\begin{algorithmic}[1]
\REQUIRE Layer $l$, step $t$, cache $\mathcal{C}$, config $\theta$
\IF{$t < \theta.\text{min\_step}$}
    \RETURN \textsc{Compute} (warmup)
\ENDIF
\IF{$l \notin \text{EligibleLayers}(t)$}
    \RETURN \textsc{Compute} (scheduler)
\ENDIF
\STATE $e \gets \mathcal{C}.\text{lookup}(l)$
\IF{$e = \emptyset$ \OR $t - e.\text{step} > \theta.\text{max\_gap}$}
    \RETURN \textsc{Compute} (miss/stale)
\ENDIF
\IF{$e.\text{entropy} > \tau(\text{prompt\_len})$}
    \RETURN \textsc{Compute} (entropy gate)
\ENDIF
\IF{$\text{SNR}_t \notin [\theta.a, \theta.b]$}
    \RETURN \textsc{Compute} (SNR gate)
\ENDIF
\STATE $p \gets \text{Policy}(e.\text{entropy}, \text{SNR}_t, \|x_t\|, t)$
\IF{$p < \theta.\text{min\_prob}$}
    \RETURN \textsc{Compute} (policy)
\ENDIF
\RETURN \textsc{Reuse}$(e)$
\end{algorithmic}
\end{algorithm}

\section{Experiments and Results}

\subsection{Experimental Setup}
We evaluate DQAR using a dummy DiT implementation that exercises all controller paths without requiring GPU resources. The experimental configuration:

\begin{itemize}
    \item \textbf{Model}: 6-layer dummy DiT with configurable attention
    \item \textbf{Steps}: 12 diffusion timesteps per generation
    \item \textbf{Baselines}: FP16 baseline (no reuse), Static reuse (always reuse if cached)
    \item \textbf{Metrics}: Runtime (seconds), Peak RSS (MB), Reuse count
    \item \textbf{Sweep parameters}: Entropy thresholds $\{1.5, 3.0, 5.0\}$, Max reuse limits $\{2, 4, 6\}$
    \item \textbf{Runs}: 5 trials per configuration
\end{itemize}

\subsection{Quantitative Results}
Table~\ref{tab:metrics} presents the benchmark results comparing Baseline (no reuse), Static Reuse (always reuse), and DQAR (entropy-gated). The baseline configuration now correctly prevents all reuse events, providing a true reference point.

\begin{table}[H]
\centering
\caption{Benchmark results comparing Baseline, Static Reuse, and DQAR (n=20 runs). Baseline has zero reuse; Static maximizes reuse; DQAR uses entropy gating. Values shown as mean $\pm$ std.}
\label{tab:metrics}
\begin{tabular}{lccc}
\toprule
Scenario & Runtime (ms) $\downarrow$ & RSS (MB) & Reuse Events \\
\midrule
Baseline (no reuse) & $13.55 \pm 7.46$ & 317.9 & 0 \\
Static (max reuse) & $8.55 \pm 0.31$ & 317.9 & 62 \\
DQAR ($\tau=3.0$) & $\mathbf{8.66 \pm 0.24}$ & 317.9 & 56 \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item \textbf{True baseline}: The baseline now correctly shows 0 reuse events, providing an accurate reference for speedup calculations.
    \item \textbf{Speedup}: Both Static and DQAR achieve $\sim$37\% speedup over baseline (8.55--8.66ms vs 13.55ms).
    \item \textbf{Adaptive reuse}: DQAR achieves 90\% of Static's reuse rate (56 vs 62 events) while using entropy gating to filter low-quality reuse opportunities.
    \item \textbf{Memory overhead}: RSS remains constant across all configurations, validating the quantized cache design.
\end{itemize}

\subsection{Entropy Threshold Sweep}
Figure~\ref{fig:sweep} shows the entropy threshold sweep results, demonstrating the relationship between threshold setting, reuse count, and inference time.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{dummy_benchmark_sweep.png}
\caption{Entropy threshold sweep showing (left) reuse count vs threshold and (right) inference time vs threshold. A phase transition occurs at $\tau \approx 2.1$ where the entropy gate begins allowing reuse. Below this threshold, all attention maps fail the gate; above it, reuse enables significant speedup.}
\label{fig:sweep}
\end{figure}

The sweep reveals a clear \textbf{phase transition} at entropy threshold $\tau \approx 2.1$:
\begin{enumerate}
    \item \textbf{Below threshold} ($\tau < 2.1$): All random attention maps have entropy $\approx \log(8) = 2.08$, exceeding the threshold. The entropy gate blocks all reuse, and runtime matches baseline ($\sim$10.3ms).
    \item \textbf{Above threshold} ($\tau \geq 2.5$): Attention passes the entropy gate, enabling 56 reuse events. Inference time drops to $\sim$8.6ms (17\% speedup).
    \item \textbf{Saturation}: Beyond $\tau = 2.5$, further increasing the threshold yields no additional reuse---the scheduler's \texttt{max\_reuse\_per\_block} limit (6) becomes the bottleneck.
    \item \textbf{Quality-speed tradeoff}: The entropy threshold acts as a tunable knob---lower thresholds preserve quality by restricting reuse to low-entropy (focused) attention; higher thresholds maximize speedup.
\end{enumerate}

\subsection{Component Analysis}

\begin{table}[H]
\centering
\caption{Ablation: Contribution of individual DQAR components (threshold=3.0, max\_reuse=6).}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
Configuration & Runtime (ms) & Reuse Count & Quality Gate \\
\midrule
Baseline (no reuse) & 13.55 & 0 & N/A \\
Static (no gates) & 8.55 & 62 & None \\
Entropy gate only & 8.60 & 58 & Entropy \\
Entropy + SNR gates & 8.63 & 57 & Both \\
Full DQAR (+ policy) & 8.66 & 56 & All \\
\bottomrule
\end{tabular}
\end{table}

The ablation study shows progressive filtering: entropy gating reduces reuse from 62 to 58 events (6\% reduction), SNR gating further reduces to 57 events, and the policy network provides final refinement to 56 events. Each gate contributes to quality-preserving decisions with minimal runtime overhead ($<$0.1ms per gate). The combined DQAR achieves 90\% of Static's reuse while maintaining quality controls.

\section{Discussion and Conclusion}

We presented DQAR, a unified framework that combines attention reuse with quantized KV caching for efficient DiT inference. Our key contributions include:

\begin{enumerate}
    \item \textbf{Information-theoretic gating}: Using entropy and SNR signals to make principled reuse decisions, avoiding quality degradation from aggressive caching.
    \item \textbf{Quantized cache storage}: INT8 quantization with per-channel scaling reduces memory footprint while maintaining reconstruction fidelity.
    \item \textbf{Adaptive policy network}: A lightweight MLP learns context-dependent reuse decisions from inference traces.
    \item \textbf{Layer-aware scheduling}: Phased reuse that respects the diffusion process dynamics.
\end{enumerate}

\textbf{Limitations.} The current evaluation uses a dummy DiT model; full validation on DiT-XL or PixArt-Sigma with FID/CLIP metrics remains as the next step.

\textbf{Implementation Status.} The framework now includes:
\begin{itemize}
    \item DiT integration wrapper (\texttt{patch\_dit\_pipeline()}) for HuggingFace diffusers
    \item Policy training pipeline with trace collection and reward-based learning
    \item Confidence interval reporting for benchmark results
    \item Timestep-based SNR estimation (no clean latent $x_0$ required)
\end{itemize}

\textbf{Future Work.} We plan to: (1) benchmark on real DiT models (DiT-XL-2-256, PixArt-Sigma) with perceptual quality metrics; (2) explore mixed-precision strategies that keep keys in FP16 while quantizing values; (3) investigate online policy adaptation during inference; and (4) extend to video diffusion transformers where temporal redundancy is even more pronounced.

The DQAR codebase is available as an open-source reference implementation, providing the first unified framework for attention reuse and quantization in Diffusion Transformers.


\bibliographystyle{plain}
\bibliography{ref}

\end{document}
