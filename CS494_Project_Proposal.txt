Dynamic
and
Quan
tization-Aw
are
A
tten
tion
Reuse
for
Di\013usion
T
ransformers
Pro
ject
Prop
osal
Name:
Gautham
Sat
y
anara
y
ana
Date:
Octob
er
17,
2025
1.
In
tro
duction
Di\013usion
T
ransformers
\050DiTs\051
ha
v
e
b
ecome
a
dominan
t
arc
hitecture
for
large-sc
al
e
generativ
e
mo
deling,
com
bining
the
scalabilit
y
of
transformers
wit
h
the
denoising
di\013usion
pro
cess.
Ho
w-
ev
er,
their
inference
remains
costly
due
to
quadratic
self-atten
tion
and
large
k
ey{v
alue
\050KV\051
cac
hes
that
are
recomputed
for
eac
h
time
step.
Recen
t
w
ork,
includin
g
A
tten
ti
on
Compr
ession
for
Di\013usion
T
r
an
sf
ormer
Mo
dels
[
1
],
demon-
strated
that
atten
tion
maps
exhibit
strong
temp
oral
redu
ndancy
during
sampling,
allo
wing
for
partial
reuse
b
et
w
een
steps.
In
parallel,
PTQ4DiT
[
2
]
an
d
Q-DiT
[
3
]
addresse
d
quan
tiza-
tion
c
hallenges
unique
to
di\013usion
transformers|sp
eci\014cally
,
salien
t
activ
ation
c
hannels
and
timestep-dep
enden
t
distributions.
Building
on
these
adv
ances,
this
pro
j
e
ct
p
rop
oses
a
uni\014ed
framew
ork,
Dynamic
and
Quan
tization-Aw
are
A
tten
tion
Reuse
\050DQAR\051
,
that
com-
bines
en
tr
op
y-
and
SNR-based
atten
tion
reuse
with
lo
w-bit
quan
tized
KV
cac
hing.
2.
R
ela
te
d
W
ork
2.1.
A
tten
tion
Compression
and
Reuse
[
1
]
prop
osed
three
strategies|Windo
w
A
tten
tion
with
Res
id
ual
Sharing,
A
tten
tion
S
haring
Across
Timeste
p
s
,
and
CF
G
Bran
c
h
Sharing|to
exploit
redundancy
in
DiTs.
Their
metho
d
ac
hiev
ed
signi\014can
t
FLOP
reductions
while
main
taining
image
qualit
y
,
sho
wing
that
atten
tion
maps
can
b
e
reused
safely
on
c
e
con
v
ergence
b
e
gi
ns
.
2.2.
Quan
tization
in
Di\013usion
T
ransformers
PTQ4DiT
[
2
]
in
tro
du
c
ed
c
hannel-wise
salience
balancing
\050CSB\051
and
Sp
earman's
\032
{guided
cal-
ibration
to
stabilize
p
ost-training
q
uan
tization
across
timesteps.
Q
-DiT
[
3
]
extended
this
b
y
prop
osing
automatic
gran
ularit
y
allo
cation
and
dynamic
activ
ation
quan
tization.
Outside
DiTs,
E\016cientDM
[
4
]
exp
lored
quan
tization-a
w
are
\014ne-tuning
for
general
d
i\013usion
mo
dels,
s
h
o
wing
that
h
ybrid
quan
tization
can
yield
strong
e\016ciency{qualit
y
trade-o\013s
.
2.3.
Motiv
ation
for
DQAR
Existing
approac
hes
treat
compression
and
qu
an
tization
indep
enden
tly
.
Our
framew
ork
in
te
-
grates
b
oth:
\0501\051
u
s
i
ng
information-theoretic
signals
\050en
trop
y
and
SNR\051
to
decide
when
to
reuse
cac
hed
atten
tion;
an
d
\0502\051
quan
tizing
reused
KV
tensors
to
minimize
VRAM
and
I/O
cost.
This
coupling
creates
an
adaptiv
e,
compute-e
\016cien
t
inference
pip
eline
for
DiTs.
1
3.
Des
ign
a
nd
M
etho
dology
3.1.
1.
En
trop
y
&
SNR-Based
Reuse
Gate
A
t
eac
h
timestep
t
,
w
e
compu
te
the
atten
tion
en
trop
y
H
t
=
\000
1
H
T
H
X
h
=1
X
i;j
A
\050
h
\051
t
\050
i;
j
\051
l
og
\050
A
\050
h
\051
t
\050
i;
j
\051
+
\017
\051
and
laten
t
signal-to-noise
ratio
SNR
t
=
k
x
0
k
2
2
k
x
t
\000
x
0
k
2
2
+
\017
:
A
tten
tion
is
reused
only
if
H
t
<
\034
\050
p
\051
and
SNR
t
2
[
a;
b
],
where
\034
\050
p
\051
adapts
to
prompt
length.
3.2.
2.
Quan
tized
KV
Cac
hing
Cac
hed
K/V
tensors
are
stored
in
8-bit
i
n
teger
form:
K
q
=
clip
\022
round
\022
K
s
K
\023
;
\000
127
;
127
\023
;
s
K
=
max
j
K
j
127
:
Tw
o
mo
des
are
supp
orted:
\0501\051
p
er-tensor
scaling
for
sp
eed,
and
\0502\051
p
er-c
hannel
scaling
for
\014delit
y
.
A
mixed-pr
e
cision
v
arian
t
k
eeps
K
in
FP16
and
quan
tizes
V
.
3.3.
3.
Dynamic
Reuse
P
olicy
A
ligh
t
w
eigh
t
MLP
\050
<
0
:
5M
parameters\051
predicts
the
probabilit
y
of
reuse:
p
reuse
=
P
\022
\050concat\050
H
t
;
SNR
t
;
k
x
t
k
2
;
t
\051\051
:
The
p
olicy
is
trained
o\017ine
on
cac
h
e
d
inference
traces
lab
eled
b
y
p
erformance
impact,
enabling
data-driv
en
decisions
without
mo
difying
the
di\013usion
mo
del.
3.4.
4.
La
y
er
Sc
heduling
Early
timesteps
reuse
only
shallo
w
blo
c
ks
\050where
en
trop
y
is
high
and
signal
w
eak\051,
while
later
timesteps
reuse
deep
er
blo
c
ks
\050where
con
v
ergence
stabil
iz
es
atten
tion\051.
This
sc
heduling
aligns
reuse
in
tensit
y
with
the
di\013usion
timeline.
3.5.
5.
In
tegration
DQAR
mo
dules
in
tegrate
directly
in
to
standard
DDIM
or
DPMSolv
er
samplers.
Eac
h
di\013usion
step:
1.
Computes
en
trop
y
and
SNR
for
the
curren
t
step.
2.
Consults
the
reuse
gate
or
p
olicy
.
3.
If
appro
v
ed,
retriev
es
qu
an
tized
K/V
and
dequ
an
tizes
them.
4.
Otherwise,
recomputes
atten
tion
and
up
dates
cac
hes.
2
4.
E
xp
erimen
tal
Setup
\210
Base
Mo
del:
DiT-Base
or
DiT-XL
\050Hugging
F
ace
/
Op
enDiT\051
\210
Hardw
are:
A100
GPU
\050Go
ogle
Colab
P
ro\051
\210
Data:
64{256
text
prompts
from
COCO-2017
or
LAION
\210
Baselines:
FP16
DiT,
A
tten
tion
Compression
\050static\051,
PTQ4DiT
\050quan
tized\051
\210
Metrics:
FID,
CLIP
score,
run
time,
VRAM
usage
\210
Ablations:
Reuse
o\013
/
En
trop
y
gate
/
Quan
t
cac
he
/
P
olicy
head
5.
E
xp
ected
Outcomes
\210
Ac
hiev
e
\025
25%
inference
sp
eedup
or
\025
20%
VRAM
reduction
with
\024
1
FID
degradation.
\210
Empirically
map
the
relationship
b
et
w
een
atten
tion
en
trop
y
,
SNR,
and
qualit
y
stabilit
y
.
\210
Release
the
\014rst
op
en-source
co
de
unifying
reuse
and
quan
tization
for
DiTs.
References
[1]
Y
uan,
Zhihang;
Zhang,
Hanling;
Pu,
Lu;
Ning,
Xuefei;
Zhang,
Linfeng;
Zhao,
Tianc
hen;
Y
an,
Shengen;
Dai,
Guohao;
W
ang,
Y
u.
A
tten
tion
Compression
for
Di\013usion
T
ransformer
Mo
dels
.
In
Neu
rIPS
,
2024.
[2]
W
u,
Jun
yi;
W
ang,
Hao
xuan;
Shang,
Y
uzhang;
Shah,
Mu
barak;
Y
an,
Y
an.
PTQ4DiT:
P
ost-training
Quan
tization
for
Di\013usion
T
ransformers
.
In
NeurIPS
,
2024.
https:
//arxiv.org/abs/2405.16005
[3]
Chen,
Lei;
Meng,
Y
uan;
T
ang,
Chen;
Ma,
Xin
z
h
u;
Jian
g,
Jingy
an;
W
ang,
Xin;
W
ang,
Zhi;
Zh
u,
W
en
wu.
Q-DiT:
Accurate
P
ost-T
raining
Quan
tization
for
Di\013usion
T
rans-
formers
.
arXiv
pr
eprint
arXiv:2406.17343
,
2024.
[4]
He,
Y
efei;
Liu,
Jing;
W
u,
W
eijia;
Zhou,
Hong;
Zh
uang,
Bohan.
E\016cien
tDM:
E\016cien
t
Quan
tization-Aw
are
Fine-T
uning
of
L
o
w
-
B
i
t
Di\013usion
Mo
dels
.
arXiv
pr
eprint
arXiv:2310.03270
,
2023.
3